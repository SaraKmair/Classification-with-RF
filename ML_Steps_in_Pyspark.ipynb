{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Steps in Pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYctchOROTBG9WGOHj/da1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtdRJyy0AHq4"
      },
      "outputs": [],
      "source": [
        "#import libraries \n",
        "import pyspark.sql.functions as f\n",
        "import pandas as pd \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read data frame "
      ],
      "metadata": {
        "id": "vJ1aW20NBFAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning "
      ],
      "metadata": {
        "id": "4qfls-_3MZfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with Nulls"
      ],
      "metadata": {
        "id": "-FhMuLMhBwoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oMxiYmuPBnFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the missing value % in each column \n",
        "missing_df = df.select([(f.count(f.when(f.isnan(c) |f.col(c).isNull(), c))/f.count(f.lit(1))).alias(c) for c in df.columns])"
      ],
      "metadata": {
        "id": "n_hv0gjKAmDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the missing values dataframe to pandas and transpose the dataframe \n",
        "missing_pd = missing_df.toPandas()\n",
        "missing_pd.T"
      ],
      "metadata": {
        "id": "SXsV1IRMBQJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replace null values with 0\n",
        "df = df.na.fill(0)"
      ],
      "metadata": {
        "id": "rDvPk3OYBaQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with categorical variables "
      ],
      "metadata": {
        "id": "DFF6H9ZHBvjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#apply one hot encoder to categorical variables \n",
        "#we can not apply one hot encoder to string columns directly\n",
        "#first convert string to numeric \n",
        "#string indexer after that apply one hot encoder \n",
        "\n",
        "from pyspark.ml import Pipeline \n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler \n",
        "\n",
        "#categoric_features are the columns with categorical values \n",
        "#convert categorical with one hot encoder to numeric values \n",
        "indexers = [StringIndexer(inputCol = c, outputCol = \"{0}_indexed\".format(c))\n",
        "            for c in categoric_features\n",
        "            ]\n",
        "\n",
        "encoders = [\n",
        "            OneHotEncoder(\n",
        "                inputCol = indexer.getOutputCol(),\n",
        "                outputCol = \"{0}_encoded\".format(indexer.getOutputCol())\n",
        "            ) for indexer in indexers \n",
        "]\n",
        "\n",
        "\n",
        "assemblers = VectorAssembler(\n",
        "    inputCols = [encoder.getOutputCol() for encoder in encoders], outputCol = \"cat_features\"\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages = indexers +encoders +[assembler]) #+[assembler] add this to get one vector for all cat features \n",
        "\n",
        "df_encoded = pipeline.fit(df).transform(df)"
      ],
      "metadata": {
        "id": "B9KifVYJBlo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with Outliers (Trim method) "
      ],
      "metadata": {
        "id": "8PSePWzNDwMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate upper and lower bound for each column \n",
        "# create 3% and 97% bounds as cut-offs to trim the data for outliers (the cut off is flexible based on the dataset)\n",
        "\n",
        "bounds = {\n",
        "    c:dict(\n",
        "        zip([\"q3\", \"q97\"], df.approxQuantile(c, [0.03, 0.97], 0.025))\n",
        "    ) for c in df.columns if c in numeric_features #apply it only on the columns with numeric features\n",
        "}\n",
        "\n",
        "#get the upper and lower bound for each column\n",
        "\n",
        "for c in bounds:\n",
        "  bounds[c]['lower'] = bounds[c]['q3'] #lower bound at 3%\n",
        "  bounds[c]['upper'] = bounds[c]['q97'] #upper bound at 97%\n",
        "\n",
        "\n",
        "  #create an indicator for outliers 0 within bounds 1 outside of bounds and add outlier column indicator for each column (this is to check the percent of \n",
        "  #outliers in each column )\n",
        "\n",
        "\n",
        "  df_out = df.select(\n",
        "      \"*\",\n",
        "      *[\n",
        "        f.when(\n",
        "            f.col(c).between(bounds[c]['lower'], bounds[c]['upper']\n",
        "        ), 0).otherwise(1).alias(c+\"_out\")\n",
        "        for c in df.columns if c in numeric_features\n",
        "\n",
        "      ]\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  #get the outliers \n",
        "  outliers = [col for col in df_out.columns if '_out' in col]\n",
        "\n",
        "\n",
        "  #get percent of outliers in each column \n",
        "  exp = {x: \"mean\" for x in outliers}\n",
        "  pd_out = df_out.agg(exp).toPandas()\n",
        "\n",
        "  #show the percent of outliers in each column \n",
        "  pd_out.T\n",
        "\n",
        "\n",
        "  #outliers above upper limit \n",
        "  max_for_replacement = [c for c, v in df.select([\n",
        "                                                  f.count(f.when(f.col(c) > bounds[c]['upper'], 1)).alias(c) for c in numeric_feautres \n",
        "\n",
        "  ]).first().asDict().items() if v]\n",
        "\n",
        "\n",
        "  #set outliers above upper limit to the upper limit \n",
        "  df = df.select(\n",
        "      \"*\",\n",
        "      *[f.when(f.col(c) > bounds[c]['upper'], bounds[c]['upper'])\n",
        "      .otherwise(df[c]).alias(c+\"_maxtrim\")\n",
        "      for c in df.columns if c in max_for_replacement]\n",
        "  )\n",
        "\n",
        "  df = df.drop(*max_for_replacement)\n",
        "\n",
        "  for name in df.schema.names:\n",
        "    df = df.withColumnRenamed(name, name.replace('_maxtrim', ''))\n",
        "\n",
        "\n",
        "cols_min = df.columns \n",
        "\n",
        "#outliers below lower limit \n",
        "min_for_replacement = [c for c, v in df.select([\n",
        "                                                f.count(f.when(f.col(c) < bounds[c]['lower'], 1)).alias(c) for c in numeric_features\n",
        "]).first().asDict().items() if v ]\n",
        "\n",
        "\n",
        "df = df.select(\n",
        "    \"*\",\n",
        "    *[f.when(f.col(c) < bounds[c]['lower'], bounds [c]['lower'])\n",
        "    .otherwise(df[c]).alias(c+\"_mintrim\")\n",
        "    for c in df.columns if c in min_for_replacement])\n",
        "\n",
        "df = df.drop(*min_for_replacement)\n",
        "\n",
        "for name in df.schema.names:\n",
        "  df = df.withColumnRenamed(name, name.replace('_mintrim', ''))\n",
        "\n",
        "\n",
        "  #save table \n",
        "  df.write.mode('overwrite').saveAsTable('table_name')\n"
      ],
      "metadata": {
        "id": "ispTYtHGDtBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**End of Data Cleaning**"
      ],
      "metadata": {
        "id": "7c1WUAmiMizl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering \n"
      ],
      "metadata": {
        "id": "wC0OWByCMnRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log Transformation "
      ],
      "metadata": {
        "id": "uJjpEOmIMIUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#log transformation can't be applied on 0s and negative value \n",
        "# to deal with 0 we take log(x+1) instead of log(x)\n",
        "#to deal with negative value we take the minimum value in the column and we add it to all the values in that column so we get\n",
        "#rid of all negative value by replacing log(x) with log(x+a) where a is the minimum values of each column \n",
        "\n",
        "#creating dictionary for all the minimum values \n",
        "d = {}\n",
        "for col in numeric_features:\n",
        "  d[col] = df.agg({col :\"min\"}).collect()[0][0]\n",
        "\n",
        "df_log = df\n",
        "\n",
        "#looping through the columns and adding the constant values to the column \n",
        "for col in numeric_features:\n",
        "  df_log = df_log.withColumn(col, f.log(df_log[col] - d[col] +1))"
      ],
      "metadata": {
        "id": "PCFq31FLM4OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MinMax Scaler "
      ],
      "metadata": {
        "id": "KyVrnF_dN_1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#minmax scaler rescale each feature to a specific range between [0,1]\n",
        "#define col to scale before running this code below \n",
        "scaler = MinMaxScaler(min = 0, max = 1, inputCol = \"features\", outputCol = \"scaledFeatures\")\n",
        "assembler = VectorAssembler().setInputCols(coltoscale).setOutputCol(\"features\")\n",
        "df_transformed = assembler.transform(df_log) #the dataset after applying log transformation \n",
        "scalerModel = scaler.fit(df_transformed.select(\"features\"))\n",
        "scaled_df = scalerModel.transform(df_transformed)"
      ],
      "metadata": {
        "id": "dMZGWNa6NkaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Scaler"
      ],
      "metadata": {
        "id": "Lv47ZhsWQnB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#standard scaler transforms a dataset of vector rows, normalizing each feature to have unit standard deviation and 0 mean \n",
        "#standard scaler assumes that the distribution of the data is normal \n",
        "\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "scaler = StandardScaler(inputCol = \"features\", outputCol = \"scaledFeatures\",\n",
        "                        withstd = True, withMean = False)\n",
        "\n",
        "assembler = VectorAssembler().setInputCols(coltoscal).setOutputCol(\"features\")\n",
        "df_transformed = assembler.transform(df_log)\n",
        "scalerModel = scaler.fit(df_transformed.select(\"features\"))\n",
        "scaledData = scalerModel.transform(df_transformed)"
      ],
      "metadata": {
        "id": "Gdb-vQ8cQp78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robust Scaler"
      ],
      "metadata": {
        "id": "_tbVWZu-PE9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#robust scaler transforms a dataset of vector rows, removing the median and scaling the data according to a specific quantile range \n",
        "#by default IQR. it is similar to standard scaler however the mendian and the quantile range are used instead of \n",
        "#the mean and standard deviation which make it robust to outliers \n",
        "\n",
        "assemblers = [VectorAssembler(inputCols = [col], outputCol = col+\"_vec\") for col in coltoscal]\n",
        "scaler = RobustScaler(inputCol = \"features\", outputCol = \"scaledFeatures\", withscaling = True, withcentering = False, lower = 0.03, upper = 0.97)\n",
        "pipeline = Pipeline(stages = assemblers, scaler)\n",
        "scalerModel - pipeline.fit(df)\n",
        "\n",
        "#transform each feature to have unit quantile range \n",
        "scaledData = scalerModel.transform(df)\n"
      ],
      "metadata": {
        "id": "uaG6nBDOPGf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing labels "
      ],
      "metadata": {
        "id": "6ZDQI7lnR0hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#label indexer \n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline \n",
        "\n",
        "labels = StringIndexer(inputCol = \"label\", outputCol = \"label_indexed\")\n",
        "pipeline = Pipeline(stages = [labels])\n",
        "df = pipeline.fit(df).transform(df)"
      ],
      "metadata": {
        "id": "MpbbffySR3ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Test Split"
      ],
      "metadata": {
        "id": "KmuqY7bSSP7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split dataset into 70% training and 30% testing \n",
        "(trainingData, testData) = df.randomSplit([0.7, 0.3])"
      ],
      "metadata": {
        "id": "TxekZ09vSTR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dealing with imbalanced data \n",
        "\n",
        "#oversampling \n",
        "a = range(0)\n",
        "oversample_df = df.withColumn(\"dummy\", f.explode(f.array([f.lit(x) for x in a ]))).drop('dummy')\n",
        "df_done = df.unionAll(oversample_df)\n",
        "\n",
        "#undersampline \n",
        "undersample_df = df.sample(False, 0.5) #take 50% of the sample without replacement \n"
      ],
      "metadata": {
        "id": "_lkZfZKPaPoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Model "
      ],
      "metadata": {
        "id": "RXoaUAgPa1oW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier \n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
        "\n",
        "rf = RandomForestClassifier(featurescol = \"scaledFeatures\", labelCol = 'label_indexed', numTrees = 'number', maxDepth = 'number')\n",
        "\n",
        "#chain random forest in a pipeline\n",
        "pipeline = Pipeline(stages = [rf])\n",
        "\n",
        "#train model \n",
        "rfmodel = pipeline.fit(df)\n",
        "\n",
        "#make predictions on train set \n",
        "trainpredictions = rfmodel.transform(trainingData)\n",
        "testpredcitions = rfmodel.trasform(testData)\n",
        "\n",
        "#apply the same steps above on the validation (out of time test set)\n",
        "#the validation set should be held out of the training and balancing \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CDB1Z2oOa3j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation \n"
      ],
      "metadata": {
        "id": "ULfYB-I8b7DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate multiclass classification model \n",
        "evaluator = MulticlassClassificationEvaluator(labelCol = \"label_indexed\")\n",
        "print('F1-Score', evaluator.evaluate(testPredictions, {evaluator.metricName: 'f1'}))\n",
        "print('Precision', evaluator.evaluate(testPredictions, {evaluator.metricName: 'weightedPrecision'}))\n",
        "print('Recall', evaluator.evaluate(testPredictions, {evaluator.metricName: 'weightedRecall'}))\n",
        "print('Accuracy', evaluator.evaluate(testPredictions, {evaluator.metricName: 'accuracy'}))\n",
        "\n",
        "#evaluate in train set \n",
        "evaluator = MulticlassClassificationEvaluator(labelCol = \"label_indexed\")\n",
        "print('F1-Score', evaluator.evaluate(trainPredictions, {evaluator.metricName: 'f1'}))\n",
        "print('Precision', evaluator.evaluate(trainPredictions, {evaluator.metricName: 'weightedPrecision'}))\n",
        "print('Recall', evaluator.evaluate(trainPredictions, {evaluator.metricName: 'weightedRecall'}))\n",
        "print('Accuracy', evaluator.evaluate(trainPredictions, {evaluator.metricName: 'accuracy'}))"
      ],
      "metadata": {
        "id": "uKrqqhc0b8hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get classification report \n",
        "\n",
        "y_true = spark.sql(\"\"\"select label_indexed from saved_results_in_table\"\"\").toPandas()\n",
        "y_pred = spark.sql(\"\"\"select prediction from saved_results_in_table\"\"\").toPandas()\n",
        "\n",
        "#classification report by class \n",
        "from sklearn.metrics import classification_report \n",
        "target_names = [\"class{}\".format(i) for i in range(4)] #4 here is the number of classes change as it fits\n",
        "print(classification_report(y_true, y_pred, target_names = target_names))\n",
        "\n",
        "#get roc_auc score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer \n",
        "\n",
        "#create function for roc_auc in multiclass \n",
        "def multiclass_roc_auc_score(y_test,y_pred, average = \"macro\"): #change average as it fits used macro here becuase of class imbalance\n",
        " lb = LabelBinarizer()\n",
        " lb.fit(y_test)\n",
        " y_test = lb.transform(y_test)\n",
        " y_pred = lb.transform(y_pred)\n",
        " return roc_auc_score(y_test, y_pred, average = average)\n",
        "\n",
        "print('ROC AUC Score:' , multiclass_roc_auc_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "#get confusion matrix \n",
        "from sklearn.metrics import confusion_matrix \n",
        "cnf_matrix = confusion_matrix(y_true, y_pred, labels = range(4))\n",
        "\n",
        "cnf_matrix"
      ],
      "metadata": {
        "id": "CvLX69dncuyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model to use later \n",
        "\n",
        "model_path = \"///\"\n",
        "\n",
        "rfmodel.write.overwrite().save(model_path)"
      ],
      "metadata": {
        "id": "4uccwGtjegNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning "
      ],
      "metadata": {
        "id": "sxHoyrrQesf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier()\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol = \"prediction\", metricName = \"f1\")\n",
        "paramGrid = (paramGridBuilder().\\\n",
        "             addGrid(rf.numTrees, [x1, x2, x3]) #x1, x2, x3 are values for the model to test and pick the best based on performance\n",
        "             .addGrid(rf.maxDepth, [x1, x2, x3])\n",
        "             .build())\n",
        "\n",
        "crossval = CrossValidator(\n",
        "    estimator = rf,\n",
        "    estimatorparamMaps = paramGrid,\n",
        "    evaluator = evaluator,\n",
        "    numFolds = 4 #change the value based on the number of folds \n",
        ")\n",
        "\n",
        "model = crossval.fit(testData)\n",
        "bestmodel = model.bestmodel\n",
        "\n",
        "\n",
        "#get best params \n",
        "model.getEstimatorParamMaps()[np.argmax(model.avgMetrics)]"
      ],
      "metadata": {
        "id": "S5VUir36er7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance "
      ],
      "metadata": {
        "id": "wlmc3q_6fvzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfmodel.stages[-1].featureImportances\n",
        "\n",
        "#try techiniques like shap values which is better than feature importance \n"
      ],
      "metadata": {
        "id": "sJuzS3MIfu1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0Bp8Pb0SdBsL"
      }
    }
  ]
}